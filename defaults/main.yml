---

# Defaults for ansible-role-slurm
#
slurm_packages:
 - slurm-plugins
 - slurm
 - slurm-perlapi
 - slurm-slurmdb-direct
 - slurm-sql
 - slurm-lua
 - slurm-devel
 - slurm-pam_slurm
 - slurm-sjstat
 - slurm-slurmdbd
 - slurm-torque
 - slurm-munge
 - slurm-sjobexit
# - slurm-spank-x11
#

slurm_node_packages:
 - slurm
 - slurm-devel
 - slurm-lua
 - slurm-munge
 - slurm-pam_slurm
 - slurm-perlapi
 - slurm-plugins
 - slurm-sjobexit
 - slurm-sjstat
 - slurm-sql
 - slurm-torque
# - slurm-spank-x11-0.2.5-1.x86_64
# - slurm-spank-x11-debuginfo-0.2.5-1.x86_64

slurm_dep_packages:
 - "xterm"
 - "sssd"
 - "gcc" 
 - "make"
 - "gcc-c++"
 - "wget"
 - "vim"
 - "man"
 - "rpm-build"
 - "pam"
 - "pam-devel"
 - "hwloc"
 - "hwloc-devel"
 - "munge"
 - "munge-devel"
 - "munge-libs"
 - "readline-devel"
 - "openssl-devel"
 - "perl-ExtUtils-MakeMaker"
 - "lua"
 - "lua-devel"
 - "lua-posix"
 - "lua-filesystem"

admingroup: "admin"
# By setting slurm_munge_key_from_nfs to True we copy the munge.key from slurm_munge_key_nfs
# If it's False then we copy it from files/ where ansible runs
# This is the default. So with ansible-pull we set slurm_munge_key_nfs to True
slurm_munge_key_from_nfs: False
# By default we copy the munge.key to NFS
slurm_munge_key_to_nfs: True
slurm_munge_key_nfs_dir: "/home/{{ admingroup }}"
slurm_munge_key_nfs: "{{ slurm_munge_key_nfs_dir }}/munge.key"
#

#####
fgci_install: True
fgci_slurmrepo_version: "fgcislurm1508"
siteName: "io"
nodeBase: "{{ siteName }}"
nodeGpuBase: "gpu"
#set slurm_mysql_passwords to something. If this variable is not set the role will fail.
#slurm_mysql_password: "CHANGEME"
# slurm_version is only used when grabbing the source 
slurm_version: "15.08.3"
# slurm_type (service, compute or submit) should be set with group_vars
slurm_type: ""
slurm_build: False
slurm_log_dir: "/var/log/slurm"
slurm_tmp_dir: "/opt/slurmdb/tmp/slurmd"
slurmd_tmp_dir: "{{ slurm_tmp_dir }}"
slurm_state_dir: "/opt/slurmdb/tmp"

# If the current node is a nis server, make sure the slurm user and
# group exist
nis_server: False

##
slurm_service_node: "{{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}"
slurm_accounting_storage_host: "{{ slurm_service_node }}"
slurm_accounting_storage_type: "accounting_storage/slurmdbd"
slurm_accounting_storage_loc: "slurm_acct_db"
slurm_accounting_storage_user: "slurm"
slurm_clustername: "{{ siteName | default('test_cluster')}}"
slurm_compute_nodes: "{{ nodeBase }}[1-4]"
slurm_compute_realmemory: "126000"
slurm_compute_sockets: "2"
slurm_compute_corespersocket: "12"
slurm_compute_threadspercore: "2"
slurm_node_state: "UNKNOWN"
slurm_with_gpu: True
slurm_gpu_nodes: "{{ nodeGpuBase }}"
##
slurm_first_job_id: "2230000"
slurm_max_job_count: "30000"
slurm_scheduler_parameters: "bf_max_job_test=300,bf_max_job_part=200,bf_max_job_user=30,defer,bf_continue,bf_window=7200,bf_resolution=1800"
slurm_select_type_parameters: "CR_Core_Memory"
slurm_priority_type: "priority/multifactor"
slurm_priority_decayhalflife: "14-0"
slurm_priority_favorsmall: "NO"
slurm_priority_weight_fairshare: "10000000"
slurm_priority_weight_age: "10000"
slurm_priority_weight_partition: "0"
slurm_priority_weight_jobsize: "10000"
slurm_priority_weight_qos: "10000"
slurm_priority_maxage: "7"
slurm_accounting_storage_enforce: "limits,qos"
slurm_healthcheck_program: "/usr/sbin/nhc"
slurm_healthcheck_interval: "300"
slurm_cgroup_automount : "yes"
slurm_cgroup_constrain_cores: "yes"
slurm_cgroup_cpu_affinity: "yes"
slurm_cgroup_constrain_ram: "no" # Use the very strict cgroup-based memory tracker?
slurm_cgroup_allowedramspace: "100.1" # At which % of requested memory should the process get killed?
slurm_cgroup_constrain_swap: "no"
slurm_cgroup_taskaffinity: "yes" 
slurm_cgroup_allowedswapspace: "120" # % of memory allocation that can be phys mem+swap
slurm_cgroup_constrain_devices: "no" 
slurm_cgroup_min_ram: "500" # always allocate this much memory to each job
slurm_cgroup_release_agent_dir: "/etc/slurm/cgroup"

