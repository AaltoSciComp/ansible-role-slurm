# {{ ansible_managed }}
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=test_cluster
ControlMachine={{ hostvars[groups['slurm_service'][0]]['ansible_hostname']  }}
ControlAddr={{ hostvars[groups['slurm_service'][0]]['ansible_hostname'] }}
#BackupController=service02
#BackupAddr=service02
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/tmp/slurmstate
SlurmdSpoolDir=/tmp/slurmd
SwitchType=switch/none
MpiDefault=none
MpiParams=ports=12000-12999
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
#ProctrackType=proctrack/cgroup
#PluginDir=
CacheGroups=0
FirstJobId=2230000
ReturnToService=1
MaxJobCount=30000
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
PropagateResourceLimitsExcept=MEMLOCK,RLIMIT_AS,RLIMIT_CPU,RLIMIT_NPROC,RLIMIT_CORE,RLIMIT_DATA,RLIMIT_RSS,STACK
EnforcePartLimits=YES
#Prolog=/etc/slurm/prolog
#Epilog=/etc/slurm/epilog
#PrologSlurmctld=/etc/slurm/slurmctld_prolog
#EpilogSlurmctld=/etc/slurm/slurmctld_epilog
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=task/cgroup
TaskPlugin=task/none
#TrackWCKey=no
#TreeWidth=50
#TmpFs=
UsePAM=1
RebootProgram=/sbin/reboot
#
#HealthCheckInterval=1800
#HealthCheckProgram=/etc/slurm/health_check
#HealthCheckNodeState=IDLE

#
#
#GresTypes=mic,gpu
#GresTypes=gpu
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=600
InactiveLimit=1800
MinJobAge=300
MessageTimeout=99
KillWait=10
CompleteWait=12
Waittime=0
KillOnBadExit=1
KeepAliveTime=60
#
# SCHEDULING
SchedulerType=sched/backfill
SchedulerParameters     = bf_max_job_user=30,bf_continue,bf_interval=60,bf_resolution=180,max_job_bf=300,defer_rpc_cnt=10
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK
DefMemPerCPU=512
FastSchedule=2
PriorityType=priority/multifactor
PriorityDecayHalfLife=7-0
#PriorityUsageResetPeriod=14-0
PriorityWeightFairshare=1000
PriorityWeightAge=500
PriorityWeightPartition=1000
PriorityWeightJobSize=1000
PriorityMaxAge=6-0
Licenses=mdcs:256
#MaxSubmitJobs=2000
#
# LOGGING
SlurmctldDebug=4
SlurmctldLogFile=/var/log/slurm/Slurmctld.log
SlurmdDebug=3
#DebugFlags=backfill
SlurmdLogFile=/var/log/slurm/Slurmd.log
#JobCompType=jobcomp/filetxt
#JobCompLoc=/slurmdb/log/jobcomp.log
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=energy=30,task=30
AcctGatherEnergyType=acct_gather_energy/rapl
AcctGatherNodeFreq=30
#JobAcctGatherFrequency=task=30
#

#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ hostvars[groups['slurm_service'][0]]['inventory_hostname']  }}
AccountingStorageLoc=slurm_acct_db
#AccountingStoragePass=
AccountingStorageUser=slurm
AccountingStorageEnforce=associations,limits


#JobSubmitPlugins=lua

#
# TOPOLOGY
#
#TopologyPlugin=topology/tree
# COMPUTE NODES
NodeName={{ hostvars[groups['slurm_compute'][0]]['inventory_hostname']  }} Sockets=1 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3500 TmpDisk=1000 Weight=10

PartitionName=serial Nodes={{ hostvars[groups['slurm_compute'][0]]['inventory_hostname']  }} Default=YES MaxNodes=1 Shared=No DefaultTime=5 MaxTime=3-0 State=UP

