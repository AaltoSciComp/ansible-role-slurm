# FGCI slurm.conf
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ slurm_clustername }}
ControlMachine={{ slurm_service_node }}
{% if slurm_control_addr is defined %}
ControlAddr={{ slurm_control_addr }}
{% endif %}
#BackupController=service02
#BackupAddr=service02
#
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation={{ slurm_state_dir }}
#SlurmdSpoolDir={{ slurm_tmp_dir }}
SwitchType={{ slurm_switch_type }}
MpiDefault={{ slurm_mpi_default }}
MpiParams={{ slurm_mpi_params }}
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType={{ slurm_proctrack_type }}
#PluginDir=
CacheGroups=1
FirstJobId={{ slurm_first_job_id }}
ReturnToService={{ slurm_return_to_service }}
MaxJobCount={{ slurm_max_job_count }}
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
PropagateResourceLimitsExcept={{ slurm_propagate_resource_limits_except }}
EnforcePartLimits={{ slurm_enforce_part_limits }}
{% if slurm_jobsubmitplugins is defined %}
JobSubmitPlugins={{ slurm_jobsubmitplugins }}
{% endif %}
#Prolog=/etc/slurm/prolog
Epilog=/usr/local/libexec/slurm/epilog.d/*
#PrologSlurmctld=/etc/slurm/slurmctld_prolog
#EpilogSlurmctld=/etc/slurm/slurmctld_epilog
#SrunProlog=
#SrunEpilog=
#TaskProlog=
TaskEpilog=/usr/bin/epilog
TaskPlugin=task/cgroup
#TaskPlugin=task/none
#TrackWCKey=no
#TreeWidth=50
#TmpFs=
UsePAM={{ slurm_usepam }}
RebootProgram=/sbin/reboot
#
HealthCheckInterval={{ slurm_healthcheck_interval }}
HealthCheckProgram={{ slurm_healthcheck_program }}
#HealthCheckNodeState=IDLE

#
#
#GresTypes=mic,gpu
#GresTypes=gpu
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=600
InactiveLimit=0
MinJobAge=30
MessageTimeout=30
KillWait=30
CompleteWait=12
Waittime=0
KillOnBadExit=1
KeepAliveTime=60
OverTimeLimit=60
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerParameters     = bf_max_job_user=30,bf_continue,bf_interval=60,bf_resolution=180,max_job_bf=300,defer_rpc_cnt=10
SchedulerParameters={{ slurm_scheduler_parameters }}
#
#SchedulerAuth=
SchedulerPort=7321
#SchedulerRootFilter=
SelectType={{ slurm_select_type }}
SelectTypeParameters={{ slurm_select_type_parameters }}
#DefMemPerCPU=512
FastSchedule={{ slurm_fast_schedule }}
PriorityType={{ slurm_priority_type }}
{% if slurm_priority_type == "priority/multifactor" %}
PriorityFlags={{ slurm_priority_flags }}
{% endif %}
PriorityDecayHalfLife={{ slurm_priority_decayhalflife }}
PriorityFavorSmall={{ slurm_priority_favorsmall }}
#PriorityUsageResetPeriod=14-0
PriorityWeightFairshare={{Â slurm_priority_weight_fairshare }}
PriorityWeightAge={{ slurm_priority_weight_age }}
PriorityWeightPartition={{ slurm_priority_weight_partition }}
PriorityWeightJobSize={{ slurm_priority_weight_jobsize }}
PriorityWeightQOS={{ slurm_priority_weight_qos }}
PriorityMaxAge={{ slurm_priority_maxage }}
#Licenses=mdcs:256
#MaxSubmitJobs=2000
#
# LOGGING
SlurmctldDebug=3
#SlurmctldLogFile={{ slurm_log_dir }}/slurmctld.log
SlurmdDebug=3
#DebugFlags=backfill
#SlurmdLogFile={{ slurm_log_dir }}/slurmd.log
JobCompLoc={{ slurm_log_dir }}/slurm_jobcomp.log
JobCompType=jobcomp/filetxt
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=energy=60,task=60
AcctGatherEnergyType=acct_gather_energy/rapl
AcctGatherNodeFreq=60
#JobAcctGatherFrequency=task=30
#

#
AccountingStorageType={{ slurm_accounting_storage_type }}
AccountingStorageHost={{ slurm_accounting_storage_host }}
AccountingStorageLoc={{ slurm_accounting_storage_loc }}
#AccountingStoragePass=
AccountingStorageUser={{ slurm_accounting_storage_user }}
AccountingStorageEnforce={{ slurm_accounting_storage_enforce }}


#
# TOPOLOGY
#
{% if slurm_topology_plugin is defined %}
TopologyPlugin={{ slurm_topology_plugin }}
{% else %}
#TopologyPlugin=topology/tree
{% endif %}

# COMPUTE NODES
{% if slurm_nodelist is defined %}
{% for nodelist in slurm_nodelist %}
{{ nodelist }}
{% endfor %}
{% else %}
NodeName={{ slurm_compute_nodes }} RealMemory={{ slurm_compute_realmemory }} Sockets={{ slurm_compute_sockets }} CoresPerSocket={{ slurm_compute_corespersocket }} ThreadsPerCore={{ slurm_compute_threadspercore }} State={{ slurm_node_state }}
# If slurm_with_gpu is True then add some nodes and a partition for them.
{% if slurm_with_gpu %}
NodeName={{ slurm_gpu_nodes }} RealMemory=250000 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 State=UNKNOWN feature=gpu
{% endif %}
{% endif %}

# partitions
{% if slurm_partitionlist is defined %}
{% for partition in slurm_partitionlist %}
{{ partition }}
{% endfor %}
{% else %}
{% if slurm_with_gpu %}
PartitionName=normal Nodes={{ slurm_compute_nodes }},{{ slurm_gpu_nodes }} Default=YES MaxTime=INFINITE State=UP DefaultTime=2:00:00
{% else %}
PartitionName=normal Nodes={{ slurm_compute_nodes }} Default=YES MaxTime=INFINITE State=UP DefaultTime=2:00:00
{% endif %}
PartitionName=test Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00
PartitionName=grid Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00
{% endif %}
